{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "First, we must ensure that the Watson Python SDK is installed and ready to use, we'll then import the SDK as well as the pandas library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install ibm-watson==5.1.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ibm_watson import DiscoveryV1\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize Watson Discovery\n",
    "\n",
    "Now we'll initialize Watson Discovery using our login credentials. In order to obtain these, create a Watson Discovery services with your IBM Cloud account, and generate new credentials."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "disco_authenticator = IAMAuthenticator('{discovery_apikey}')\n",
    "discovery = DiscoveryV1(\n",
    "    version='{discovery_version}',\n",
    "    authenticator=disco_authenticator\n",
    ")\n",
    "\n",
    "discovery.set_service_url('{discovery_url}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the query\n",
    "\n",
    "There are a few elements to querying Watson Discovery news, I'll break down each of the elements.\n",
    "\n",
    "`environment_id`: `system` just denotes that we're using the system environment\n",
    "\n",
    "`collection_id`: We want to query the news collection\n",
    "\n",
    "`natural_language_query`: string for free text search\n",
    "\n",
    "`offset`: The number of results (documents) to skip\n",
    "\n",
    "`count`: The number of results (documents) to return \n",
    "\n",
    "***note: Count and offset is the way pagination of results is implemented, the maximum of total results (offset + count) cannot exceed 10,000***\n",
    "\n",
    "`aggregation`: This is a analytic query of the results set\n",
    "\n",
    "`filter`: The query for matching documents\n",
    "\n",
    "`return_`: What items to actually return to us for our use\n",
    "\n",
    "**For more information, check out the [query reference](https://cloud.ibm.com/docs/services/discovery/query-reference.html#query-reference)**\n",
    "\n",
    "\n",
    "We are using DEFAULT_COUNT of 50, the maximum you can query at once, and incrementing until we've captured all available documents or hit 10000 (the maximum)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_results = []\n",
    "\n",
    "TOTAL_NUM_RESULTS = 50;\n",
    "DEFAULT_COUNT = 50;\n",
    "offset = 0;\n",
    "\n",
    "STRING_TO_SEARCH = 'IBM NLP';\n",
    "\n",
    "while offset + DEFAULT_COUNT <= TOTAL_NUM_RESULTS:\n",
    "    try:\n",
    "        discovery_results = discovery.query(environment_id='system',\n",
    "                                 collection_id='news-en',\n",
    "                                 natural_language_query ='IBM NLP',\n",
    "                                 offset=offset,\n",
    "                                 count=DEFAULT_COUNT,\n",
    "                                 return_=\"url,author,title,text\"\n",
    "                                 ).get_result()\n",
    "        \n",
    "        # If the results are empty, stop querying\n",
    "        if not discovery_results['results']:\n",
    "            break\n",
    "        \n",
    "        # Add results to all_results and increment offset\n",
    "        for result in discovery_results['results']:\n",
    "            all_results.append(result)\n",
    "        \n",
    "        offset += offset + DEFAULT_COUNT\n",
    "    except Exception as e:\n",
    "        print(\"ERROR DETECTED\")\n",
    "        print(e)\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(all_results))\n",
    "all_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add Enrichments\n",
    "\n",
    "Now that we've queried Watson Discovery, we want to analyze in detail each article.\n",
    "\n",
    "So we are going to use the `url` info that we obtained for each result to perform a detailed query with **Natural Language Understanding**.\n",
    "\n",
    "Note that this is different in comparison to the `enrich_text.sentiment` field that can be returned by Watson Discovery call, as it \"only\" analyzes what is contained within the `text` field"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize Watson Natural Language Understanding \n",
    "\n",
    "Now we'll initialize Watson Natural Language Understanding using our login credentials. In order to obtain these, create a Watson Natural Language Understanding services with your IBM Cloud account, and generate new credentials."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator # previously imported\n",
    "\n",
    "nlu_authenticator = IAMAuthenticator('{nlu_apikey}')\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='{nlu_version}',\n",
    "    authenticator=nlu_authenticator\n",
    ")\n",
    "\n",
    "natural_language_understanding.set_service_url('{nlu_url}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the features you want to extract with NLU.\n",
    "For example, let's activate the **sentiment** analysis and the **keywords** extraction, and pass the url extracted for each result as a query parameter to the service"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example test\n",
    "Perform a simple API call to Natural Language Understanding service"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions, KeywordsOptions\n",
    "\n",
    "nlu_results = natural_language_understanding.analyze(\n",
    "    url=\"www.ibm.com\",\n",
    "    features=Features(\n",
    "        sentiment=SentimentOptions(document=True),\n",
    "        keywords=KeywordsOptions(emotion=True, limit=8))\n",
    ").get_result()\n",
    "\n",
    "print(json.dumps(nlu_results, indent=2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for result in all_results:\n",
    "    print(\"analyzing url\",result['url'])\n",
    "    try:\n",
    "        nlu_results = natural_language_understanding.analyze(\n",
    "            url=result['url'],\n",
    "            features=Features(\n",
    "                sentiment=SentimentOptions(document=True),\n",
    "                keywords=KeywordsOptions(limit=8))\n",
    "        ).get_result()\n",
    "        \n",
    "        sentiment_score = None\n",
    "        keywords_from_url = ''\n",
    "        if 'sentiment' in nlu_results:\n",
    "            sentiment_score = nlu_results['sentiment']['document']['score']\n",
    "        if 'keywords' in nlu_results and len(nlu_results['keywords']) > 0:\n",
    "            for key in nlu_results['keywords']:\n",
    "                keywords_from_url += key['text'] + ','\n",
    "        result['sentiment'] = sentiment_score\n",
    "        result['keywords'] = keywords_from_url\n",
    "    except Exception as e:\n",
    "        print(\"error finding enrichments for\",result['url'])\n",
    "        print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(json.dumps(all_results, indent=2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}